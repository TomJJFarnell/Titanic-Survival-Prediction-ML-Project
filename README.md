ğŸš¢ Titanic Survival Prediction â€“ Ensemble Modeling Approach

In this notebook, we tackle the classic Titanic survival prediction problem using a structured and model-driven approach. Rather than relying on a single algorithm, we compare multiple machine learning models and ultimately combine them through ensembling to improve accuracy and generalization.

â¸»

ğŸ§  Key Steps in This Notebook:

ğŸ§¹ Data Cleaning & Feature Engineering
	â€¢	Extracted meaningful features such as Title, FamilySize, IsAlone, and CabinLetter
	â€¢	Encoded categorical variables using label encoding
	â€¢	Handled missing values (e.g., imputing Age and Fare with the mean)

ğŸ¤– Model Training

Trained and evaluated three high-performing tree-based models:
	â€¢	Random Forest â€“ A classic ensemble of decision trees using bagging
	â€¢	XGBoost â€“ A gradient boosting model optimized for speed and performance
	â€¢	HistGradientBoosting (HGB) â€“ A fast, histogram-based gradient boosting algorithm built into scikit-learn

ğŸ”— Model Ensembling (Stacking)

After comparing the performance of individual models, we combined them using a stacking ensemble:
	â€¢	The predictions of the base models (Random Forest, XGBoost, and HGB) were fed into a Logistic Regression meta-model
	â€¢	This stacked model learned to leverage the strengths of each base learner to produce a final prediction

ğŸ“¤ Final Prediction & Submission
	â€¢	Trained the ensemble model on the full training dataset
	â€¢	Predicted outcomes on the test set
	â€¢	Submitted results in the required format for the Titanic Kaggle competition
âœ… Current best accuracy: ~75%

â¸»

ğŸ” Whatâ€™s Next

To improve the accuracy score, Iâ€™ve been researching other approaches and plan to refine this solution in a future notebook. Specifically:
	â€¢	Use MICE imputation for more accurate handling of missing data (instead of mean imputation or dropping rows)
	â€¢	Switch to One-Hot Encoding to simplify categorical variable handling
	â€¢	Focus on a single strong model, most likely Random Forest, to reduce complexity and boost interpretability

